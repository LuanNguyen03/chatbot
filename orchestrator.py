import psycopg2, os, requests
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# Database connection with fallback
try:
    conn = psycopg2.connect(os.getenv("POSTGRES_URI"))
    cur = conn.cursor()
    print("‚úÖ Database connected successfully")
except Exception as e:
    print(f"‚ö†Ô∏è Database connection failed: {e}")
    conn = None
    cur = None

def embed_query(query: str):
    # Skip embedding due to quota limits - return dummy embedding
    print("‚ö†Ô∏è Skipping embedding due to API limits")
    return [0.0] * 768

def query_policies(embedding):
    if not cur:
        print("‚ö†Ô∏è Database not available, skipping policy search")
        return []
    
    try:
        cur.execute("SELECT text, filename, title, page_numbers FROM policies ORDER BY embedding <-> %s LIMIT 5", (embedding,))
        return cur.fetchall()
    except Exception as e:
        print(f"Database query error: {e}")
        # Return empty result if table doesn't exist or pgvector not installed
        return []

def fetch_chat_history(user_id: str, limit: int = 20):
    if not cur:
        return []
    
    try:
        cur.execute("""
            SELECT ch.role, ch.message, ch.created_at
            FROM chat_history ch
            JOIN sessions s ON s.session_id = ch.session_id
            WHERE s.user_id = %s
            ORDER BY ch.created_at DESC
            LIMIT %s
        """, (user_id, limit))
        rows = cur.fetchall()
        rows = rows[::-1]
        return [{"role": r[0], "message": r[1], "created_at": r[2].isoformat() if r[2] else None} for r in rows]
    except Exception as e:
        print(f"Chat history error: {e}")
        return []

def get_customer_info(user_id):
    base = os.getenv("CUSTOMER_API_URL")
    token = os.getenv("CUSTOMER_API_TOKEN", "")
    if not base:
        return None
    headers = {"Authorization": f"Bearer {token}"} if token else {}
    try:
        r = requests.get(f"{base}/{user_id}", headers=headers, timeout=10)
        if r.status_code == 200:
            return r.json()
    except Exception:
        pass
    return None

def generate_answer(question: str, context: str):
    model = genai.GenerativeModel("gemini-1.5-flash")  # Updated model name
    
    # Check if we have specific data or using general knowledge
    has_specific_data = "Policies DB:" in context
    has_history = "Recent Chat History:" in context
    
    if has_specific_data:
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp c·ªßa HDBank.

NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu ch√≠nh th·ª©c c·ªßa HDBank.

C·∫§U TR√öC TR·∫†L·ªúI:
üìã **TH√îNG TIN CH√çNH:**
[Th√¥ng tin ch√≠nh v·ªÅ c√¢u h·ªèi]

üí° **CHI TI·∫æT:**
[Gi·∫£i th√≠ch chi ti·∫øt v√† v√≠ d·ª• c·ª• th·ªÉ]

üìû **LI√äN H·ªÜ:**
ƒê·ªÉ bi·∫øt th√™m th√¥ng tin chi ti·∫øt, qu√Ω kh√°ch vui l√≤ng li√™n h·ªá HDBank qua:
- Hotline: 1900 6060
- Website: hdbank.com.vn
- ·ª®ng d·ª•ng HD Bank mobile

QUY T·∫ÆC:
- S·ª≠ d·ª•ng emoji ƒë·ªÉ l√†m r√µ n·ªôi dung
- Tr√¨nh b√†y ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu
- D·ª±a tr√™n d·ªØ li·ªáu ch√≠nh th·ª©c trong CONTEXT

CONTEXT:
{context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi theo c·∫•u tr√∫c tr√™n:"""
    else:
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp c·ªßa HDBank.

NHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ HDBank v√† d·ªãch v·ª• ng√¢n h√†ng d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng qu√°t.

C·∫§U TR√öC TR·∫†L·ªúI:
üè¶ **V·ªÄ HDBANK:**
[Th√¥ng tin t·ªïng qu√°t v·ªÅ HDBank li√™n quan ƒë·∫øn c√¢u h·ªèi]

üí° **TH√îNG TIN CHUNG:**
[Ki·∫øn th·ª©c chung v·ªÅ d·ªãch v·ª• ng√¢n h√†ng ƒë∆∞·ª£c h·ªèi]

‚ö†Ô∏è **L∆∞U √ù:**
Th√¥ng tin tr√™n mang t√≠nh ch·∫•t tham kh·∫£o. ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c v√† c·∫≠p nh·∫≠t nh·∫•t, qu√Ω kh√°ch vui l√≤ng:

üìû **LI√äN H·ªÜ HDBANK:**
- Hotline: 1900 6060  
- Website: hdbank.com.vn
- ·ª®ng d·ª•ng HD Bank mobile
- ƒê·∫øn tr·ª±c ti·∫øp c√°c chi nh√°nh HDBank

QUY T·∫ÆC:
- T·∫≠p trung v√†o HDBank v√† ng√†nh ng√¢n h√†ng Vi·ªát Nam
- S·ª≠ d·ª•ng emoji ƒë·ªÉ d·ªÖ ƒë·ªçc
- Khuy·∫øn kh√≠ch kh√°ch h√†ng li√™n h·ªá HDBank ƒë·ªÉ c√≥ th√¥ng tin ch√≠nh th√°c

{f"NG·ªÆ C·∫¢NH: {context}" if context and "General" not in context else ""}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi theo c·∫•u tr√∫c tr√™n:"""
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"‚ùå Gemini generation error: {e}")
        return "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau."

def suggest_followup_questions(question: str, answer: str, context: str):
    model = genai.GenerativeModel("gemini-1.5-flash")  # Updated model name
    prompt = f"""D·ª±a tr√™n cu·ªôc tr√≤ chuy·ªán v·ªÅ HDBank, h√£y ƒë·ªÅ xu·∫•t 3 c√¢u h·ªèi ti·∫øp theo ph√π h·ª£p.

CU·ªòC TR√í CHUY·ªÜN:
Kh√°ch h√†ng h·ªèi: "{question}"
HDBank Bot tr·∫£ l·ªùi: "{answer}"

Y√äU C·∫¶U:
- ƒê·ªÅ xu·∫•t 3 c√¢u h·ªèi li√™n quan ƒë·∫øn HDBank v√† d·ªãch v·ª• ng√¢n h√†ng
- C√°c c√¢u h·ªèi n√™n logic v√† h·ªØu √≠ch cho kh√°ch h√†ng
- Vi·∫øt ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu

FORMAT TR·∫†L·ªúI:
1. [C√¢u h·ªèi 1]
2. [C√¢u h·ªèi 2]  
3. [C√¢u h·ªèi 3]

ƒê·ªÅ xu·∫•t:"""
    
    try:
        txt = model.generate_content(prompt).text or ""
        return [line.strip() for line in txt.split("\n") if line.strip().startswith(("1","2","3"))][:3]
    except Exception as e:
        print(f"‚ö†Ô∏è Followup questions error: {e}")
        # Return default followup questions for HDBank
        return [
            "1. HDBank c√≥ nh·ªØng s·∫£n ph·∫©m ti·∫øt ki·ªám n√†o?",
            "2. L√†m th·∫ø n√†o ƒë·ªÉ m·ªü t√†i kho·∫£n t·∫°i HDBank?", 
            "3. L√£i su·∫•t vay t·∫°i HDBank nh∆∞ th·∫ø n√†o?"
        ]

def orchestrator(question: str, user_id: str = "default_user"):
    print(f"üîç Processing question: {question}")
    ctx = []
    
    # Skip database and embedding for now due to API limits
    print("‚ö†Ô∏è Skipping database search due to API quota limits")
    
    # Step 1: Get chat history
    history = fetch_chat_history(user_id, limit=20)
    if history:
        htxt = "\n".join([f"[{h['role']}] {h['message']}" for h in history])
        ctx.append("Recent Chat History:\n" + htxt)
        print("‚úÖ Added chat history to context")
    
    # Step 2: Get customer info
    info = get_customer_info(user_id)
    if info:
        ctx.append("Customer API:\n" + str(info))
        print("‚úÖ Added customer info to context")
    
    # Step 3: Use Gemini general knowledge for banking questions
    print("üè¶ Using Gemini general knowledge for HDBank questions")
    ctx.append(f"General Banking Knowledge: C√¢u h·ªèi v·ªÅ {question}")
    
    print(f"üìù Final context sources: {len(ctx)}")
    full = "\n\n".join(ctx)
    ans = generate_answer(question, full)
    
    # Simplified followup questions
    try:
        sug = suggest_followup_questions(question, ans, full)
    except Exception as e:
        print(f"‚ö†Ô∏è Followup questions failed: {e}")
        sug = []
    
    return ans, sug
